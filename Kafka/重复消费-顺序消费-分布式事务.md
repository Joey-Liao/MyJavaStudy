# 消息的重复消费

你下个单**支付成功**你就发个消息出去，我们上面那个活动的开发人员就**监听**你的**支付成功消息**，我监听到你这个订单成功支付的消息，那我就去我活动GMV表里给你加上去，听到这里大家可能**觉得顺理成章**。

![image-20230516153033491](D:\Typora\workspace\Kafka\重复消费-顺序消费-分布式事务.assets\image-20230516153033491.png)

**但是**我告诉大家一般消息队列的使用，我们都是有**重试机制**的，就是说我下游的业务发生异常了，我会抛出异常并且要求你**重新发一次**。

我这个活动这里发生错误，你要求重发肯定没问题。但是大家**仔细想一下**问题在哪里？

是的，不止你一个人监听这个消息啊，**还有别的服务也在监听**，他们也会失败啊，他一失败他也要求重发，但是你这里其实是成功的，重发了，你的钱不就加了两次了？

## 如何解决-接口幂等

通俗了讲就是你**同样的参数调用我这个接口，调用多少次结果都是一个**，你加GMV同一个订单号你加一次是多少钱，你加N次都还是多少钱。

大致处理流程如下：

![image-20230516153259782](D:\Typora\workspace\Kafka\重复消费-顺序消费-分布式事务.assets\image-20230516153259782.png)



### 接口幂等-强校验

将多个接口放在一个事务中，成功一起成功，失败一起失败。

并且每次消息过来都要拿着**订单号+业务场景这样的唯一标识**（比是天猫双十一活动）去流水表查，看看有没有这条流水，有就直接return不要走下面的流程了，没有就执行后面的逻辑。

用伪代码表示：

![image-20230516153717866](D:\Typora\workspace\Kafka\重复消费-顺序消费-分布式事务.assets\image-20230516153717866.png)

### 接口幂等-弱校验

一些不重要的场景，我们就可以把这个id+场景唯一标识作为**Redis**的key，放到缓存里面失效时间看你场景，**一定时间内**的这个消息就去Redis判断。



# 消息的顺序消费

一般都是**同个业务场景下不同几个操作的消息同时过去**，本身顺序是对的，但是你发出去的时候同时发出去了，消费的时候却乱掉了，这样就有问题了。

## 如何解决-RocketMQ

生产者消费者一般需要保证顺序消息的话，可能就是一个业务场景下的，比如订单的**创建、支付、发货、收货。**

那这些东西是不是一个订单号呢？一个订单的肯定是一个订单号的说，那简单了呀。

**一个topic下有多个队列**，为了保证发送有序，**RocketMQ**提供了**MessageQueueSelector**队列选择机制，他有三种实现:

![image-20230516154312637](D:\Typora\workspace\Kafka\重复消费-顺序消费-分布式事务.assets\image-20230516154312637.png)

我们可以使用Hash取模，让同一个订单发送到同一个队列中，再使用同步发送，又因为RocketMQ里topic队列中是FIFO，就能保证插入时的顺序性



# 分布式事务

大家可以想一下，你下单流程可能涉及到10多个环节，你下单付钱都成功了，但是你优惠券扣减失败了，积分新增失败了，前者公司会被薅羊毛，后者用户会不开心，但是**这些都在不同的服务怎么保证大家都成功呢**？--**分布式事务**

## 2pc 两段式提交

![image-20230516170622405](D:\Typora\workspace\Kafka\重复消费-顺序消费-分布式事务.assets\image-20230516170622405.png)

通过消息中间件协调多个系统，在两个系统操作事务的时候都锁定资源但是不提交事务，等两者都准备好了，告诉消息中间件，然后再分别提交事务。

缺陷：如果A系统事务提交成功了，但是B系统在提交的时候网络波动或者各种原因提交失败了，其实还是会失败的。

## 最终一致性



![image-20230516170949201](D:\Typora\workspace\Kafka\重复消费-顺序消费-分布式事务.assets\image-20230516170949201.png)

整个流程中，我们能保证是：

- 业务主动方本地事务提交失败，业务被动方不会收到消息的投递。
- 只要业务主动方本地事务执行成功，那么消息服务一定会投递消息给下游的业务被动方，并最终保证业务被动方一定能成功消费该消息（消费成功或失败，即最终一定会有一个最终态）。
